{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample all platforms to have the same number of data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twitter      488167\n",
       "reddit        72135\n",
       "instagram     15652\n",
       "youtube       11304\n",
       "Name: platform, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('full_dataset.csv')\n",
    "df.platform.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positivity_normalized'] = (df.positive_words - df.negative_words)/df.word_count\n",
    "df['intensity_normalized'] = (df.positive_words + df.negative_words)/df.word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = df.loc[df.platform=='twitter'].sample(11303)\n",
    "df_reddit = df.loc[df.platform=='reddit'].sample(11303)\n",
    "df_youtube = df.loc[df.platform=='youtube']\n",
    "df_instagram = df.loc[df.platform=='instagram'].sample(11303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = pd.concat([df_twitter, df_reddit, df_youtube, df_instagram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "youtube      2261\n",
       "instagram    2261\n",
       "twitter      2261\n",
       "reddit       2260\n",
       "Name: platform, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df_small.copy(), \n",
    "                                     shuffle=True, \n",
    "                                     test_size=.2, \n",
    "                                     random_state=404,\n",
    "                                    stratify=df_small.platform)\n",
    "\n",
    "df_test.platform.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('small_train_dataset.csv')\n",
    "df_test.to_csv('small_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positivity is the positive_words and negative_words balance.\n",
    "Intensity is the sum of pos and neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>platform</th>\n",
       "      <th>positive_words</th>\n",
       "      <th>negative_words</th>\n",
       "      <th>length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>male_words</th>\n",
       "      <th>female_words</th>\n",
       "      <th>afinn</th>\n",
       "      <th>positivity_normalized</th>\n",
       "      <th>intensity_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>531834</th>\n",
       "      <td>531834</td>\n",
       "      <td>Grandpa?</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325553</th>\n",
       "      <td>325553</td>\n",
       "      <td>The sooner we as an Ummah live in accordance t...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530669</th>\n",
       "      <td>530669</td>\n",
       "      <td>High school? College?</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580659</th>\n",
       "      <td>580659</td>\n",
       "      <td>Damn I love this song and this music video</td>\n",
       "      <td>youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576082</th>\n",
       "      <td>576082</td>\n",
       "      <td>He is so messy with the spices lol</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "531834      531834                                           Grandpa?   \n",
       "325553      325553  The sooner we as an Ummah live in accordance t...   \n",
       "530669      530669                              High school? College?   \n",
       "580659      580659         Damn I love this song and this music video   \n",
       "576082      576082                 He is so messy with the spices lol   \n",
       "\n",
       "       platform  positive_words  negative_words  length  word_count  \\\n",
       "531834   reddit               0               0       8           1   \n",
       "325553  twitter               0               0     115          23   \n",
       "530669   reddit               0               0      21           3   \n",
       "580659  youtube               1               1      42           9   \n",
       "576082  youtube               0               1      34           8   \n",
       "\n",
       "        male_words  female_words  afinn  positivity_normalized  \\\n",
       "531834           0             0    0.0                  0.000   \n",
       "325553           0             0    2.0                  0.000   \n",
       "530669           0             0    0.0                  0.000   \n",
       "580659           0             0    1.0                  0.000   \n",
       "576082           2             0    3.0                 -0.125   \n",
       "\n",
       "        intensity_normalized  \n",
       "531834              0.000000  \n",
       "325553              0.000000  \n",
       "530669              0.000000  \n",
       "580659              0.222222  \n",
       "576082              0.125000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single feature logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('small_train_dataset.csv')\n",
    "df_test = pd.read_csv('small_test_dataset.csv')\n",
    "\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits, shuffle=True, random_state = 404)\n",
    "features = ['word_count', 'afinn', 'positivity_normalized', 'intensity_normalized', 'male_words', 'female_words']\n",
    "\n",
    "## Make your array of zeros to hold the accuracies\n",
    "log_reg_accs = np.zeros((n_splits, len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  75   80 1342  312]\n",
      " [ 180  144  754  731]\n",
      " [ 192  183 1101  332]\n",
      " [ 130  101 1019  558]]\n",
      "[[ 558 1183    0   68]\n",
      " [ 544 1126    0  139]\n",
      " [ 557 1118    0  133]\n",
      " [ 623 1057    0  128]]\n",
      "[[ 490 1294   12   13]\n",
      " [ 473 1335    0    1]\n",
      " [ 462 1345    0    1]\n",
      " [ 480 1292   10   26]]\n",
      "[[ 546    5 1234   24]\n",
      " [ 640    2 1146   21]\n",
      " [ 579    1 1193   35]\n",
      " [ 615    1 1170   22]]\n",
      "[[   0    0 1700  109]\n",
      " [   0    0 1717   92]\n",
      " [   0    0 1737   71]\n",
      " [   0    0 1642  166]]\n",
      "[[  95    0 1714    0]\n",
      " [  72    0 1737    0]\n",
      " [  43    0 1765    0]\n",
      " [  51    0 1757    0]]\n",
      "[[ 111   61 1289  348]\n",
      " [ 168  146  765  730]\n",
      " [ 197  192 1093  326]\n",
      " [ 170   90 1006  542]]\n",
      "[[ 608 1127    0   74]\n",
      " [ 551 1121    0  137]\n",
      " [ 564 1114    0  130]\n",
      " [ 606 1086    0  116]]\n",
      "[[ 568 1221   12    8]\n",
      " [ 513 1296    0    0]\n",
      " [ 442 1366    0    0]\n",
      " [ 478 1295   20   15]]\n",
      "[[ 575    2 1218   14]\n",
      " [ 667    2 1108   32]\n",
      " [ 587    0 1191   30]\n",
      " [ 609    3 1169   27]]\n",
      "[[   0    0 1704  105]\n",
      " [   0    0 1687  122]\n",
      " [   0    0 1727   81]\n",
      " [   0    0 1649  159]]\n",
      "[[  86    0 1723    0]\n",
      " [  68    0 1741    0]\n",
      " [  40    0 1768    0]\n",
      " [  55    0 1753    0]]\n",
      "[[  92   79 1314  323]\n",
      " [ 178  149  747  734]\n",
      " [ 185  152 1151  321]\n",
      " [ 146  100  994  569]]\n",
      "[[ 572 1159    0   77]\n",
      " [ 577 1095    0  136]\n",
      " [ 568 1129    0  112]\n",
      " [ 619 1068    0  122]]\n",
      "[[ 520 1274    2   12]\n",
      " [ 530 1278    0    0]\n",
      " [ 465 1343    0    1]\n",
      " [ 499 1282   11   17]]\n",
      "[[ 576   15 1197   20]\n",
      " [ 661   28 1086   33]\n",
      " [ 627   30 1131   21]\n",
      " [ 665   26 1094   24]]\n",
      "[[   0    0 1718   90]\n",
      " [   0    0 1692  116]\n",
      " [   0    0 1743   66]\n",
      " [   0    0 1649  160]]\n",
      "[[  82    0 1726    0]\n",
      " [  71    0 1737    0]\n",
      " [  33    0 1776    0]\n",
      " [  61    0 1748    0]]\n",
      "[[  52  110 1304  342]\n",
      " [  80  222  781  725]\n",
      " [ 115  239 1135  320]\n",
      " [  54  183 1010  562]]\n",
      "[[ 567 1172    0   69]\n",
      " [ 548 1113    0  147]\n",
      " [ 545 1133    0  131]\n",
      " [ 572 1113    0  124]]\n",
      "[[ 541 1245   12   10]\n",
      " [ 478 1330    0    0]\n",
      " [ 434 1375    0    0]\n",
      " [ 469 1318    8   14]]\n",
      "[[ 569   10 1213   16]\n",
      " [ 661    3 1105   39]\n",
      " [ 575    0 1205   29]\n",
      " [ 589    5 1183   32]]\n",
      "[[   0    0 1683  125]\n",
      " [   0    0 1685  123]\n",
      " [   0    0 1746   63]\n",
      " [   0    0 1664  145]]\n",
      "[[  99    0 1709    0]\n",
      " [  84    0 1724    0]\n",
      " [  37    0 1772    0]\n",
      " [  79    0 1730    0]]\n",
      "[[  81   63 1342  322]\n",
      " [ 164  134  747  763]\n",
      " [ 176  169 1163  301]\n",
      " [ 134  118 1001  555]]\n",
      "[[ 598 1151    0   59]\n",
      " [ 573 1118    0  117]\n",
      " [ 611 1095    0  103]\n",
      " [ 589 1112    0  107]]\n",
      "[[ 552 1237    8   11]\n",
      " [ 500 1308    0    0]\n",
      " [ 478 1331    0    0]\n",
      " [ 483 1300    8   17]]\n",
      "[[ 592    3 1201   12]\n",
      " [ 662    4 1115   27]\n",
      " [ 602    0 1179   28]\n",
      " [ 612    6 1162   28]]\n",
      "[[   0    0 1685  123]\n",
      " [   0    0 1717   91]\n",
      " [   0    0 1749   60]\n",
      " [   0    0 1640  168]]\n",
      "[[  97    0 1711    0]\n",
      " [  68    0 1740    0]\n",
      " [  36    0 1773    0]\n",
      " [  65    0 1743    0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.26638837, 0.25085021, 0.25731986, 0.24686893, 0.26265593,\n",
       "       0.25748572])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loop through the cv splits\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(df_train,df_train.platform):\n",
    "    ## get the training and holdout sets\n",
    "    df_tt = df_train.iloc[train_index]\n",
    "    df_ho = df_train.iloc[test_index]\n",
    "    \n",
    "    ## loop through your features\n",
    "    j = 0\n",
    "    for feature in features:\n",
    "        ## Define the model\n",
    "        log_reg = LogisticRegression(penalty='none')\n",
    "        \n",
    "        ## fit the model\n",
    "        log_reg = log_reg.fit(df_tt[feature].values.reshape(-1, 1), df_tt['platform'].values)\n",
    "        \n",
    "        ## Make the prediction\n",
    "        pred = log_reg.predict(df_ho[feature].values.reshape(-1, 1))\n",
    "        \n",
    "        ## Record the accuracy on the holdout set\n",
    "        log_reg_accs[i,j] = accuracy_score(df_ho['platform'].values, pred)\n",
    "        print(confusion_matrix(df_ho['platform'].values, pred, labels=[\"instagram\", \"twitter\", \"reddit\", \"youtube\"]))\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "    \n",
    "np.mean(log_reg_accs, axis=0)\n",
    "print(log_reg_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "Original full dataset:\n",
    "\n",
    "Here we are fitting logistic regression using word_count, afinn, positivity and instensity individually. \n",
    "The confusion matrix has true labels on the rows, prediction on the columns. \n",
    "Most model just predict all points as twitter. \n",
    "The accuracy was high but the model itself is a trivial one. \n",
    "It will not generate well. \n",
    "\n",
    "We may need to use balance the dataset by undersample twitter data. \n",
    "\n",
    "small dataset: \n",
    "\n",
    "accuracy for each featuer decrease to about 26%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['word_count', 'afinn', 'positivity_normalized', 'intensity_normalized', 'male_words', 'female_words']\n",
    "df_train = pd.read_csv('small_train_dataset.csv')\n",
    "df_test = pd.read_csv('small_test_dataset.csv')\n",
    "\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits, shuffle=True, random_state = 404)\n",
    "\n",
    "## Make your array of zeros to hold the accuracies\n",
    "log_reg_accs = np.zeros((n_splits, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(df_train[features])\n",
    "df_train_scaled = scaler.transform(df_train[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'word_count': df_train_scaled[:, 0],\n",
    "         'afinn': df_train_scaled[:, 1],\n",
    "         'positivity_normalized': df_train_scaled[:, 2],\n",
    "         'intensity_normalized': df_train_scaled[:, 3],\n",
    "        'male_words': df_train_scaled[:, 4],\n",
    "        'female_words': df_train_scaled[:, 5],\n",
    "         'platform': df_train.platform}\n",
    "df_train = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[442 184 997 186]\n",
      " [400 638 542 229]\n",
      " [441 427 823 117]\n",
      " [405 341 704 358]]\n",
      "[[483 158 952 216]\n",
      " [401 616 506 286]\n",
      " [433 457 790 128]\n",
      " [390 358 708 352]]\n",
      "[[464 210 961 173]\n",
      " [422 601 505 280]\n",
      " [453 439 801 116]\n",
      " [448 367 654 340]]\n",
      "[[441 198 930 239]\n",
      " [397 622 516 273]\n",
      " [436 412 844 117]\n",
      " [372 414 702 321]]\n",
      "[[481 164 967 196]\n",
      " [399 621 499 289]\n",
      " [494 383 828 104]\n",
      " [398 385 670 355]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.31023821])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for train_index, test_index in kfold.split(df_train, df_train.platform):\n",
    "    ## get the training and holdout sets\n",
    "\n",
    "    df_tt = df_train.iloc[train_index]\n",
    "    df_ho = df_train.iloc[test_index]\n",
    "    \n",
    "    ## Define the model\n",
    "    log_reg = LogisticRegression(penalty='none')\n",
    "        \n",
    "    ## fit the model\n",
    "    log_reg = log_reg.fit(df_tt[features].values, df_tt.platform.values)\n",
    "        \n",
    "    ## Make the prediction\n",
    "    pred = log_reg.predict(df_ho[features].values)\n",
    "    ## Record the accuracy on the holdout set\n",
    "    log_reg_accs[i,0] = accuracy_score(df_ho.platform, pred)\n",
    "    print(confusion_matrix(df_ho.platform, pred, labels=[\"instagram\", \"twitter\", \"reddit\", \"youtube\"]))\n",
    "\n",
    "    i = i + 1\n",
    "    \n",
    "np.mean(log_reg_accs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two class classifier \n",
    "\n",
    "Instead of trying to classify them into the 4 bins, train model to try to distinguish one platform from the other three. \n",
    "\n",
    "First create one hot encoding for each platform, then add two together to make two-platform bins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('small_train_dataset.csv')\n",
    "df_test = pd.read_csv('small_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>afinn</th>\n",
       "      <th>positivity_normalized</th>\n",
       "      <th>intensity_normalized</th>\n",
       "      <th>male_words</th>\n",
       "      <th>female_words</th>\n",
       "      <th>platform</th>\n",
       "      <th>dum_twitter</th>\n",
       "      <th>dum_reddit</th>\n",
       "      <th>dum_instagram</th>\n",
       "      <th>dum_youtube</th>\n",
       "      <th>dum_twigram</th>\n",
       "      <th>dum_twiddit</th>\n",
       "      <th>dum_twitube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.282820</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>-1.414830</td>\n",
       "      <td>0.719725</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.230636</td>\n",
       "      <td>-0.294327</td>\n",
       "      <td>-0.181968</td>\n",
       "      <td>-0.596013</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.291204</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>-0.617096</td>\n",
       "      <td>-0.131635</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030284</td>\n",
       "      <td>-0.294327</td>\n",
       "      <td>-0.181968</td>\n",
       "      <td>-0.596013</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.543739</td>\n",
       "      <td>-0.294327</td>\n",
       "      <td>-0.181968</td>\n",
       "      <td>-0.596013</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count     afinn  positivity_normalized  intensity_normalized  \\\n",
       "0   -0.282820 -1.284547              -1.414830              0.719725   \n",
       "1   -0.230636 -0.294327              -0.181968             -0.596013   \n",
       "2    0.291204 -1.284547              -0.617096             -0.131635   \n",
       "3    0.030284 -0.294327              -0.181968             -0.596013   \n",
       "4   -0.543739 -0.294327              -0.181968             -0.596013   \n",
       "\n",
       "   male_words  female_words   platform  dum_twitter  dum_reddit  \\\n",
       "0   -0.214425     -0.165903    twitter            1           0   \n",
       "1   -0.214425     -0.165903     reddit            0           1   \n",
       "2   -0.214425     -0.165903     reddit            0           1   \n",
       "3   -0.214425     -0.165903  instagram            0           0   \n",
       "4   -0.214425     -0.165903  instagram            0           0   \n",
       "\n",
       "   dum_instagram  dum_youtube  dum_twigram  dum_twiddit  dum_twitube  \n",
       "0              0            0            1            1            1  \n",
       "1              0            0            0            1            0  \n",
       "2              0            0            0            1            0  \n",
       "3              1            0            1            0            0  \n",
       "4              1            0            1            0            0  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaled the numerical data\n",
    "scaler = preprocessing.StandardScaler().fit(df_train[features])\n",
    "df_train_scaled = scaler.transform(df_train[features])\n",
    "data = {'word_count': df_train_scaled[:, 0],\n",
    "         'afinn': df_train_scaled[:, 1],\n",
    "         'positivity_normalized': df_train_scaled[:, 2],\n",
    "         'intensity_normalized': df_train_scaled[:, 3],\n",
    "        'male_words': df_train_scaled[:, 4],\n",
    "        'female_words': df_train_scaled[:, 5],\n",
    "         'platform': df_train.platform}\n",
    "df_train = pd.DataFrame(data)\n",
    "\n",
    "# create one hot encoding\n",
    "df_train['dum_twitter'] = 0\n",
    "df_train.loc[df_train.platform=='twitter', 'dum_twitter'] = 1\n",
    "df_train['dum_reddit'] = 0\n",
    "df_train.loc[df_train.platform=='reddit', 'dum_reddit'] = 1\n",
    "df_train['dum_instagram'] = 0\n",
    "df_train.loc[df_train.platform=='instagram', 'dum_instagram'] = 1\n",
    "df_train['dum_youtube'] = 0\n",
    "df_train.loc[df_train.platform=='youtube', 'dum_youtube'] = 1\n",
    "\n",
    "df_train['dum_twigram'] = df_train.dum_twitter + df_train.dum_instagram\n",
    "df_train['dum_twiddit'] = df_train.dum_twitter + df_train.dum_reddit\n",
    "df_train['dum_twitube'] = df_train.dum_twitter + df_train.dum_youtube\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "dums = ['dum_instagram', 'dum_twitter', 'dum_reddit', 'dum_youtube',\n",
    "        'dum_twigram', 'dum_twiddit', 'dum_twitube']\n",
    "log_reg_accs = np.zeros([5, len(dums)])\n",
    "recall = np.zeros([5, len(dums)])\n",
    "j = 0\n",
    "for feature in dums:\n",
    "    i = 0\n",
    "    for train_index, test_index in kfold.split(df_train, df_train[feature]):\n",
    "        ## get the training and holdout sets\n",
    "\n",
    "        df_tt = df_train.iloc[train_index]\n",
    "        df_ho = df_train.iloc[test_index]\n",
    "\n",
    "        ## Define the model\n",
    "        log_reg = LogisticRegression(penalty='none')\n",
    "\n",
    "        ## fit the model\n",
    "        log_reg = log_reg.fit(df_tt[features].values, df_tt[feature].values)\n",
    "\n",
    "        ## Make the prediction\n",
    "        pred = log_reg.predict(df_ho[features].values)\n",
    "        ## Record the accuracy on the holdout set\n",
    "        log_reg_accs[i,j] = accuracy_score(df_ho[feature], pred)\n",
    "        conf = confusion_matrix(df_ho[feature], pred)\n",
    "        TN = conf[0,0]\n",
    "        FP = conf[0,1]\n",
    "        FN = conf[1,0]\n",
    "        TP = conf[1,1]\n",
    "        #print('recall: ', TP/(TP + FN))\n",
    "        recall[i, j] = TP/(TP + FN)\n",
    "\n",
    "        i = i + 1\n",
    "    j += 1\n",
    "    \n",
    "#print(log_reg_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75033869 0.74909454 0.74997927 0.75321409 0.49860373 0.5218005\n",
      " 0.59697539]\n",
      "[5.08733373e-03 1.10619469e-04 0.00000000e+00 2.42199157e-02\n",
      " 2.96671064e-01 6.43903788e-01 4.87115340e-01]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(log_reg_accs, axis=0))\n",
    "print(np.mean(recall,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since twitube vs redgram is the highest accuracy case, run single feature regression to see which one contributed the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>afinn</th>\n",
       "      <th>positivity_normalized</th>\n",
       "      <th>intensity_normalized</th>\n",
       "      <th>male_words</th>\n",
       "      <th>female_words</th>\n",
       "      <th>platform</th>\n",
       "      <th>dum_twitter</th>\n",
       "      <th>dum_reddit</th>\n",
       "      <th>dum_instagram</th>\n",
       "      <th>dum_youtube</th>\n",
       "      <th>dum_twigram</th>\n",
       "      <th>dum_twiddit</th>\n",
       "      <th>dum_twitube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.282820</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>-1.414830</td>\n",
       "      <td>0.719725</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.230636</td>\n",
       "      <td>-0.294327</td>\n",
       "      <td>-0.181968</td>\n",
       "      <td>-0.596013</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.291204</td>\n",
       "      <td>-1.284547</td>\n",
       "      <td>-0.617096</td>\n",
       "      <td>-0.131635</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030284</td>\n",
       "      <td>-0.294327</td>\n",
       "      <td>-0.181968</td>\n",
       "      <td>-0.596013</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.543739</td>\n",
       "      <td>-0.294327</td>\n",
       "      <td>-0.181968</td>\n",
       "      <td>-0.596013</td>\n",
       "      <td>-0.214425</td>\n",
       "      <td>-0.165903</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count     afinn  positivity_normalized  intensity_normalized  \\\n",
       "0   -0.282820 -1.284547              -1.414830              0.719725   \n",
       "1   -0.230636 -0.294327              -0.181968             -0.596013   \n",
       "2    0.291204 -1.284547              -0.617096             -0.131635   \n",
       "3    0.030284 -0.294327              -0.181968             -0.596013   \n",
       "4   -0.543739 -0.294327              -0.181968             -0.596013   \n",
       "\n",
       "   male_words  female_words   platform  dum_twitter  dum_reddit  \\\n",
       "0   -0.214425     -0.165903    twitter            1           0   \n",
       "1   -0.214425     -0.165903     reddit            0           1   \n",
       "2   -0.214425     -0.165903     reddit            0           1   \n",
       "3   -0.214425     -0.165903  instagram            0           0   \n",
       "4   -0.214425     -0.165903  instagram            0           0   \n",
       "\n",
       "   dum_instagram  dum_youtube  dum_twigram  dum_twiddit  dum_twitube  \n",
       "0              0            0            1            1            1  \n",
       "1              0            0            0            1            0  \n",
       "2              0            0            0            1            0  \n",
       "3              1            0            1            0            0  \n",
       "4              1            0            1            0            0  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['word_count', 'afinn', 'positivity_normalized', 'intensity_normalized', 'male_words', 'female_words']\n",
    "\n",
    "log_reg_accs = np.zeros([5, len(features)])\n",
    "recall = np.zeros([5, len(features)])\n",
    "j = 0\n",
    "for feature in features:\n",
    "    i = 0\n",
    "    for train_index, test_index in kfold.split(df_train, df_train['dum_twitube']):\n",
    "        ## get the training and holdout sets\n",
    "\n",
    "        df_tt = df_train.iloc[train_index]\n",
    "        df_ho = df_train.iloc[test_index]\n",
    "\n",
    "        ## Define the model\n",
    "        log_reg = LogisticRegression(penalty='none')\n",
    "\n",
    "        ## fit the model\n",
    "        log_reg = log_reg.fit(df_tt[feature].values.reshape(-1, 1), df_tt['dum_twitube'].values)\n",
    "\n",
    "        ## Make the prediction\n",
    "        pred = log_reg.predict(df_ho[feature].values.reshape(-1, 1))\n",
    "        ## Record the accuracy on the holdout set\n",
    "        log_reg_accs[i,j] = accuracy_score(df_ho['dum_twitube'], pred)\n",
    "        conf = confusion_matrix(df_ho['dum_twitube'], pred)\n",
    "        TN = conf[0,0]\n",
    "        FP = conf[0,1]\n",
    "        FN = conf[1,0]\n",
    "        TP = conf[1,1]\n",
    "        #print('recall: ', TP/(TP + FN))\n",
    "        recall[i, j] = TP/(TP + FN)\n",
    "\n",
    "        i = i + 1\n",
    "    j += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59357481 0.48972886 0.50053906 0.49937842 0.51242776 0.49937784]\n",
      "[0.46942089 0.60877074 0.72423125 0.46195001 0.07420909 0.22156266]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(log_reg_accs, axis=0))\n",
    "print(np.mean(recall,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Using all features, model was able to tell twitube from redgram with ~60% accuracy. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
